# NLtoSQL AI Backend - Enhanced Modular Architecture

A robust, production-ready FastAPI backend for converting natural language queries to SQL using multiple LLM providers with comprehensive error handling, validation, and fallback mechanisms.

## üöÄ Features

### Core Functionality
- **Multi-LLM Support**: OpenAI GPT, Anthropic Claude, DeepInfra Llama
- **Smart Fallback**: Automatic provider switching on failures
- **SQL Validation**: Safe query validation using sqlparse
- **Database Support**: CSV, Excel, SQLite file uploads
- **RESTful API**: Complete FastAPI implementation with OpenAPI docs

### Enhanced Architecture
- **Modular Design**: Separate services for different concerns
- **Session Management**: Persistent sessions with database isolation
- **Error Handling**: Comprehensive error catching and user-friendly messages
- **Logging**: Detailed logging for debugging and monitoring
- **Configuration**: Centralized config management

### API Endpoints

#### Core Endpoints
- `POST /upload` - Upload and process data files
- `POST /generate-sql` - Generate and execute SQL from natural language
- `POST /predict` - Simplified prediction endpoint for integrations
- `GET /health` - Health check with provider status
- `GET /schema/{session_id}` - Get database schema information

#### Management Endpoints
- `GET /sessions` - List all active sessions
- `POST /execute-sql/{session_id}` - Execute custom SQL queries
- `DELETE /session/{session_id}` - Clean up session data
- `GET /providers` - List available LLM providers
- `POST /providers/test` - Test all configured providers

## üì¶ Installation

### Prerequisites
- Python 3.8+
- pip or conda
- API keys for at least one LLM provider

### Quick Start

1. **Clone and setup**:
```bash
git clone <your-repo>
cd nlsql-backend
pip install -r requirements.txt
```

2. **Configure environment**:
```bash
# Copy example environment file
cp .env.example .env

# Edit .env with your API keys
OPENAI_API_KEY=your_openai_key_here
DEEPINFRA_API_KEY=your_deepinfra_key_here
ANTHROPIC_API_KEY=your_anthropic_key_here
```

3. **Run the application**:
```bash
# Development
python main.py

# Or with uvicorn
uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

4. **Access the API**:
- API Documentation: http://localhost:8000/docs
- Health Check: http://localhost:8000/health
- Alternative docs: http://localhost:8000/redoc

### Docker Deployment

```bash
# Build and run with docker-compose
docker-compose up -d

# Or build manually
docker build -t nlsql-backend .
docker run -p 8000:8000 -v $(pwd)/temp:/app/temp nlsql-backend
```

## üîß Configuration

### Environment Variables
```env
# Required: At least one LLM provider
OPENAI_API_KEY=sk-...
DEEPINFRA_API_KEY=...
ANTHROPIC_API_KEY=...

# Optional: Application settings
MAX_FILE_SIZE=50MB
TEMP_DIR=temp
LOG_LEVEL=INFO
```

### Provider Configuration
Edit `config.py` to customize:
- Model preferences
- Fallback order
- Query limits
- CORS settings

## üèóÔ∏è Architecture

### Directory Structure
```
nlsql-backend/
‚îú‚îÄ‚îÄ main.py                 # FastAPI application
‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îú‚îÄ‚îÄ llm_providers.py    # LLM provider implementations
‚îÇ   ‚îú‚îÄ‚îÄ database_manager.py # Database operations
‚îÇ   ‚îú‚îÄ‚îÄ sql_validator.py    # SQL validation and formatting
‚îÇ   ‚îî‚îÄ‚îÄ prompt_generator.py # Enhanced prompt generation
‚îú‚îÄ‚îÄ config.py               # Configuration management
‚îú‚îÄ‚îÄ requirements.txt        # Python dependencies
‚îú‚îÄ‚îÄ docker-compose.yml      # Docker deployment
‚îî‚îÄ‚îÄ temp/                   # Temporary database storage
```

### Key Components

#### LLM Providers (`services/llm_providers.py`)
- **BaseLLMProvider**: Abstract base class for all providers
- **OpenAIProvider**: OpenAI GPT integration
- **DeepInfraProvider**: DeepInfra API integration
- **AnthropicProvider**: Anthropic Claude integration
- **LLMManager**: Manages multiple providers with fallback logic

#### Database Manager (`services/database_manager.py`)
- File upload processing (CSV, Excel, SQLite)
- Schema extraction and analysis
- Safe query execution
- Session-based database isolation

#### SQL Validator (`services/sql_validator.py`)
- Query safety validation
- SQL formatting and prettification
- Dangerous operation prevention (DROP, DELETE, etc.)

#### Prompt Generator (`services/prompt_generator.py`)
- Context-aware prompt generation
- Schema-informed prompts
- Sample data inclusion for better accuracy

## üîí Security Features

### SQL Safety
- **Query Validation**: Only SELECT statements allowed
- **Dangerous Keywords**: Blocked DROP, DELETE, UPDATE, etc.
- **Syntax Checking**: sqlparse validation before execution
- **Session Isolation**: Each upload gets isolated database

### API Security
- **Input Validation**: Pydantic models for request validation
- **Error Handling**: No sensitive information in error messages
- **CORS Configuration**: Configurable allowed origins
- **Rate Limiting**: Built-in request validation

## üß™ Testing

### Manual Testing Script
```python
# test_api.py
import requests
import json

BASE_URL = "http://localhost:8000"

def test_health():
    response = requests.get(f"{BASE_URL}/health")
    print(f"Health: {response.json()}")

def test_upload(file_path):
    with open(file_path, 'rb') as f:
        files = {'file': f}
        response = requests.post(f"{BASE_URL}/upload", files=files)
    return response.json()

def test_query(session_id, query):
    data = {
        "query": query,
        "session_id": session_id,
        "model_preference": "auto"
    }
    response = requests.post(f"{BASE_URL}/generate-sql", json=data)
    return response.json()

# Run tests
if __name__ == "__main__":
    test_health()
    # upload_result = test_upload("sample_data.csv")
    # query_result = test_query(upload_result["session_id"], "Show me the top 5 records")
```

### Automated Testing
```bash
# Run with pytest (add pytest to requirements.txt)
pip install pytest pytest-asyncio httpx
pytest tests/
```

## üìä Usage Examples

### Basic Usage
```python
import requests

# 1. Upload a CSV file
files = {'file': open('data.csv', 'rb')}
upload_response = requests.post('http://localhost:8000/upload', files=files)
session_id = upload_response.json()['session_id']

# 2. Generate SQL from natural language
query_data = {
    "query": "Show me the top 10 customers by sales",
    "session_id": session_id
}
response = requests.post('http://localhost:8000/generate-sql', json=query_data)
result = response.json()

print(f"Generated SQL: {result['sql']}")
print(f"Results: {result['results']}")
```

### Advanced Usage with Model Preference
```python
# Use specific model
query_data = {
    "query": "Calculate average revenue by month",
    "session_id": session_id,
    "model_preference": "openai"  # or "anthropic", "deepinfra"
}
response = requests.post('http://localhost:8000/generate-sql', json=query_data)
```

### Integration with Frontend
```javascript
// React/Vue.js example
const generateSQL = async (query, sessionId) => {
  const response = await fetch('/api/predict', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({
      question: query,
      session_id: sessionId
    })
  });
  return await response.json();
};
```

## üîÑ API Response Examples

### Successful SQL Generation
```json
{
  "sql": "SELECT * FROM data ORDER BY sales DESC LIMIT 10",
  "formatted_sql": "SELECT *\nFROM data\nORDER BY sales DESC\nLIMIT 10",
  "results": [
    {"id": 1, "name": "John", "sales": 1000},
    {"id": 2, "name": "Jane", "sales": 900}
  ],
  "model_used": "gpt-4",
  "provider_used": "openai",
  "confidence_score": 0.95,
  "execution_time": 1.23
}
```

### Error Response
```json
{
  "sql": "SELECT * FROM users WHERE DELETE",
  "error": "SQL validation failed: Dangerous keyword 'DELETE' not allowed",
  "model_used": "gpt-4",
  "provider_used": "openai",
  "execution_time": 0.45
}
```

## üöÄ Production Deployment

### Docker Deployment
```bash
# Build and deploy
docker-compose up -d

# Check logs
docker-compose logs -f nlsql-backend

# Scale if needed
docker-compose up -d --scale nlsql-backend=3
```

### Environment Setup
```bash
# Production environment variables
export OPENAI_API_KEY=your_production_key
export CORS_ORIGINS="https://yourdomain.com,https://api.yourdomain.com"
export LOG_LEVEL=WARNING
```

### Monitoring
- Health endpoint: `/health`
- Prometheus metrics: Add `prometheus-fastapi-instrumentator`
- Logging: Structured JSON logs for production

## üõ†Ô∏è Troubleshooting

### Common Issues

1. **No LLM providers configured**
   - Check `.env` file exists and has valid API keys
   - Verify keys with `/providers/test` endpoint

2. **Database creation fails**
   - Check file format (CSV, Excel, SQLite)
   - Verify file permissions and temp directory

3. **SQL execution errors**
   - Use `/schema/{session_id}` to check table structure
   - Verify column names and data types

4. **Provider timeouts**
   - Check API key validity
   - Monitor provider status pages
   - Adjust timeout settings in config

### Debug Mode
```bash
# Run with debug logging
LOG_LEVEL=DEBUG python main.py

# Check provider status
curl http://localhost:8000/providers
```

## üîÆ Future Enhancements

- [ ] **Caching**: Redis cache for frequent queries
- [ ] **Metrics**: Prometheus metrics and Grafana dashboards
- [ ] **Authentication**: JWT token-based auth
- [ ] **Query History**: Store and replay previous queries
- [ ] **Batch Processing**: Process multiple queries
- [ ] **Export Features**: Export results to various formats
- [ ] **Advanced Validation**: More sophisticated SQL analysis
- [ ] **Model Fine-tuning**: Custom model training on user data

## üìÑ License

MIT License - see LICENSE file for details.

## ü§ù Contributing

1. Fork the repository
2. Create feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## üìû Support

- GitHub Issues: [Create an issue](https://github.com/yourusername/nlsql-backend/issues)
- Documentation: [API Docs](http://localhost:8000/docs)
- Email: your-email@domain.com